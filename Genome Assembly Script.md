---
title: "Fundulus Genome Assembly"
author: "Mathia 'Tia' Colwell"
date: "2024-10-24"
output: pdf_document
---

## Flye and Medaka 

*Flye* is a de novo assembler typically used for data generated by ONT. It takes raw reads as an input and outputs polished contigs. *Medaka* is a tool used to create consensus sequences and variant calls from nanpore data. Here, I used medaka for polishing 


```{bash}

 Set variables
INPUT_FASTQ="KC21Clean.q10.fastq "
GENOME_SIZE="1.3g"
THREADS=8
MEDAKA_MODEL="r1041_e82_400bps_sup_v5.0.0"

# Activate the shared environment for Flye and Medaka
echo "Activating environment..."
source activate medaka

# Run Flye
echo "Running Flye assembly..."
flye --nano-raw $INPUT_FASTQ --out-dir KC21Clean_flye_assembly --threads $THREADS --genome-size $GENOME_SIZE

echo "Assembly complete!"


#!/bin/bash

# Set up error handling
set -e

# Activate the environment 
source ~/miniforge3/etc/profile.d/conda.sh
conda activate medaka

# Set variables
INPUT_FASTQ="/KC21Clean.q10.fastq"
ASSEMBLY_DIR="/KC21Clean_flye_assembly"
ASSEMBLY_FASTA="${ASSEMBLY_DIR}/assembly.fasta"
OUTPUT_DIR="${ASSEMBLY_DIR}/medaka_output"
MEDAKA_MODEL="r1041_e82_400bps_sup_v5.0.0" 

# Create output directory if it doesn't exist
mkdir -p ${OUTPUT_DIR}

# Run Medaka polishing
echo "Running Medaka polishing..."
medaka_consensus -i ${INPUT_FASTQ} -d ${ASSEMBLY_FASTA} -o ${OUTPUT_DIR} -t 8 -m ${MEDAKA_MODEL} 2>&1 | tee ${OUTPUT_DIR}/medaka_log.txt

# Check if Medaka completed successfully
if [ $? -eq 0 ]; then
    echo "Medaka polishing completed successfully."
else
    echo "Medaka polishing failed. Check the log file for details."
    exit 1
fi

echo "Assembly and polishing complete!"

```

## Purge Duplications 
```{bash}
#!/bin/bash

set -e  # Exit on any error

# Set variables
ASSEMBLY="/medaka_output/consensus.fasta"
READS="/KC21Clean.q10.fastq"
OUTPUT_DIR="/purge_dups_output"
PURGE_DUPS_PATH="/purge_dups"
THREADS=20

# Create output directory
mkdir -p $OUTPUT_DIR

# Align reads to the Medaka-polished assembly
echo "Aligning reads to assembly..."
minimap2 -x map-ont -t $THREADS $ASSEMBLY $READS | gzip -c - > $OUTPUT_DIR/aligned.paf.gz

# Check if the alignment was successful
if [ ! -s "$OUTPUT_DIR/aligned.paf.gz" ]; then
    echo "Error: Alignment file is empty or not created."
    exit 1
fi

# Produce stats and cutoffs file
echo "Generating stats and cutoffs..."
$PURGE_DUPS_PATH/bin/pbcstat $OUTPUT_DIR/aligned.paf.gz
$PURGE_DUPS_PATH/bin/calcuts PB.stat > $OUTPUT_DIR/cutoffs 2> $OUTPUT_DIR/calcults.log

# Split assembly
echo "Splitting assembly..."
$PURGE_DUPS_PATH/bin/split_fa $ASSEMBLY > $OUTPUT_DIR/assembly.split.fa
echo "Split assembly file size: $(du -h $OUTPUT_DIR/assembly.split.fa | cut -f1)"

# Check if the split was successful
if [ ! -s "$OUTPUT_DIR/assembly.split.fa" ]; then
    echo "Error: Split assembly file is empty or not created."
    exit 1
fi

# Self-align
echo "Performing self-alignment..."
minimap2 -xasm5 -DP -t $THREADS $OUTPUT_DIR/assembly.split.fa $OUTPUT_DIR/assembly.split.fa | gzip -c - > $OUTPUT_DIR/assembly.split.self.paf.gz

# Check if the self-alignment was successful
if [ ! -s "$OUTPUT_DIR/assembly.split.self.paf.gz" ]; then
    echo "Error: Self-alignment file is empty or not created."
    exit 1
fi

# Purge dups and haplotigs
echo "Purging duplicates..."
$PURGE_DUPS_PATH/bin/purge_dups -2 -T $OUTPUT_DIR/cutoffs -c PB.base.cov $OUTPUT_DIR/assembly.split.self.paf.gz > $OUTPUT_DIR/dups.bed 2> $OUTPUT_DIR/purge_dups.log

# Get purged primary and haplotigs
echo "Getting purged sequences..."
$PURGE_DUPS_PATH/bin/get_seqs -e $OUTPUT_DIR/dups.bed $ASSEMBLY

# Generate histogram
echo "Generating coverage histogram..."
$PURGE_DUPS_PATH/scripts/hist_plot.py -c $OUTPUT_DIR/cutoffs PB.stat $OUTPUT_DIR/coverage_histogram.png

echo "Purge_dups process completed. Check the output files in $OUTPUT_DIR"

```

## Curate the Contigs 

```{bash}
#!/bin/bash

set -e  # Exit on any error

# Set variables
PURGED_ASSEMBLY="/purged.fa"
READS="/KC21Clean.q10.fastq"
OUTPUT_DIR="/curated_assembly"
THREADS=32

# Create output directory
mkdir -p $OUTPUT_DIR

# Remove low fragment sizes
echo "Removing contigs smaller than 10kb..."
seqkit seq -m 10000 $PURGED_ASSEMBLY > $OUTPUT_DIR/filtered_size.fasta

# Map reads to assembly
echo "Mapping reads to assembly..."
minimap2 -ax map-ont -t $THREADS $OUTPUT_DIR/filtered_size.fasta $READS > $OUTPUT_DIR/aligned.sam

# Sort and index
echo "Sorting and indexing alignment..."
samtools sort -@ $THREADS $OUTPUT_DIR/aligned.sam -o $OUTPUT_DIR/aligned.bam
samtools index $OUTPUT_DIR/aligned.bam

# Calculate coverage
echo "Calculating coverage..."
mosdepth -t $THREADS -n $OUTPUT_DIR/coverage $OUTPUT_DIR/aligned.bam

# Filter contigs based on coverage
echo "Filtering contigs based on coverage..."
# Adjusted based on the histogram: keeping contigs between 6X and 60X
awk '$4 >= 6 && $4 <= 60 {print $1}' $OUTPUT_DIR/coverage.mosdepth.summary.txt | sort | uniq > $OUTPUT_DIR/keepers.txt

# Extract final curated contigs
echo "Extracting final curated contigs..."
seqkit grep -f $OUTPUT_DIR/keepers.txt $OUTPUT_DIR/filtered_size.fasta -o $OUTPUT_DIR/curated.fa

echo "Contig curation complete. Final assembly: $OUTPUT_DIR/curated.fa"
```

## Scaffolding using Rag-tag and closing the gaps
```{bash}
#!/bin/bash

# Set variables
CURATED_ASSEMBLY="/curated.fa"
REFERENCE="/GCF_01125445.2_MU-UCD_Fhet_4.1_genomic.fna"
READS="KC21Clean.q10.fastq"
OUTPUT_DIR="/scaffolded_and_gapclosed"
THREADS=32

# Create output directory
mkdir -p $OUTPUT_DIR

# Activate conda environment
conda activate ragtag

# Scaffolding with RagTag
echo "Starting scaffolding with RagTag..."
ragtag.py scaffold $REFERENCE $CURATED_ASSEMBLY \
    -o $OUTPUT_DIR/ragtag_output \
    -t $THREADS

# Gap closing with TGS-GapCloser
echo "Starting gap closing with TGS-GapCloser..."
tgsgapcloser \
    --scaff $OUTPUT_DIR/ragtag_output/ragtag.scaffold.fasta \
    --reads $READS \
    --output $OUTPUT_DIR/gapclosed \
    --thread $THREADS \
    --ne

echo "Scaffolding and gap closing complete. Final assembly: $OUTPUT_DIR/gapclosed.scaff_seqs"

```


## Running BUSCO

```{bash}
#!/bin/bash

# Set variables
ASSEMBLY_PRE="/consensus.fasta"
ASSEMBLY_POST="/scaffolded_assembly.fasta"  
OUTPUT_DIR="//busco_results"
LINEAGE="actinopterygii_odb10"  # Appropriate lineage for Fundulus
THREADS=32

# Create output directory
mkdir -p $OUTPUT_DIR

# Activate conda environment
 conda activate busco_env 

# Run BUSCO on pre-scaffolding assembly
echo "Running BUSCO on pre-scaffolding assembly..."
busco -i $ASSEMBLY_PRE -o ${OUTPUT_DIR}/pre_scaffolding -m genome -l $LINEAGE -c $THREADS --force

# After scaffolding, run BUSCO again
# Uncomment the following lines after you've performed scaffolding

# echo "Running BUSCO on post-scaffolding assembly..."
# busco -i $ASSEMBLY_POST -o ${OUTPUT_DIR}/post_scaffolding -m genome -l $LINEAGE -c $THREADS --force

echo "BUSCO analysis complete. Results are in $OUTPUT_DIR"

```

## Name the Chromosomes 
```{bash}
#!/bin/bash

# Set my variables 
REFERENCE_GENOME="/GCF_011125445.2_MU-UCD_Fhet_4.1_genomic.fna"
MY_GENOME="/ragtag.scaffold.fasta"
OUTPUT_DIR="/new_fundulus_genome/"


# Align genome to the reference
minimap2 -ax asm5 $REFERENCE_GENOME $MY_GENOME > alignment.sam

# Convert SAM to sorted BAM
samtools view -bS alignment.sam | samtools sort -o alignment_sorted.bam

# Extract chromosome assignments
samtools view alignment_sorted.bam | awk '{print $1, $3}' | sort -u > chr_assignments.txt

# Label chromosomes 
awk '/^>/ {
    getline seq
    name = substr($0, 2)
    cmd = "grep \"^" name " \" chr_assignments.txt"
    cmd | getline assignment
    close(cmd)
    if (assignment != "") {
        split(assignment, a)
        print ">" a[2] "_" name
    } else {
        print ">Unassigned_" name
    }
    print seq
}' $MY_GENOME > $OUTPUT_DIR/new_fundulus_genome.fasta

# Clean up temporary files
rm alignment.sam alignment_sorted.bam chr_assignments.txt

echo "Labeled genome saved as $OUTPUT_DIR/new_fundulus_genome.fasta"
```

## Run Liftoff for Annotation Transfer

Liftoff to create the most accurate GFF - transfers annotations from the  MU_UCD_Fhet_4.1 reference genome

```{bash, eval = FALSE}
# Activate the Liftoff environment
mamba activate liftoff

# Run Liftoff
echo "Running Liftoff to transfer annotations..."
liftoff -g ${WHEREVER_I_PUT_THE_GENOME}/NCBI_genomic.gff -o ${OUTPUT_DIR}liftoff_out.gff -p 16 tias_fundulus_genome.fasta ${WHEREVER_I_PUT_THE_GENOME}/GCF_011125445.2_MU-UCD_Fhet_4.1_genomic.fna

echo "Liftoff completed. Annotations saved to liftoff_out.gff."
```
## Run Foreing Contaminant Screen 
Required prior to NIH ubmission.
FCS-adapt removes adapter and vector sequences
```{bash, eval=FALSE}
# Install
curl -LO https://github.com/ncbi/fcs/raw/main/dist/run_fcsadaptor.sh
chmod 755 run_fcsadaptor.sh
curl https://ftp.ncbi.nlm.nih.gov/genomes/TOOLS/FCS/releases/latest/fcs-adaptor.sif -Lo fcs-adaptor.sif
curl -LO https://github.com/ncbi/fcs/raw/main/dist/fcs.py
curl https://ftp.ncbi.nlm.nih.gov/genomes/TOOLS/FCS/releases/latest/fcs-gx.sif -Lo fcs-gx.sif
export FCS_DEFAULT_IMAGE=fcs-gx.sif

# Run
sudo ./run_fcsadaptor.sh --fasta-input tias_fundulus_genome.fasta --output-dir . --euk --container-engine singularity --image fcs-adaptor.sif

# Modify to hardmask instead of splitting contigs
vim fcs_adaptor_report
%s/ACTION_TRIM/FIX/g

# Clean the genome (~1100 bp dropped)
cat assembly.fasta | sudo python3 fcs.py clean genome --action-report ./fcs_adaptor_report.txt --output clean.fasta --contam-fasta-out contam.fasta

```
