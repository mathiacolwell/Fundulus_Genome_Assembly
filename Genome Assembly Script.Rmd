---
title: "Fundulus Genome Assembly"
author: "Mathia 'Tia' Colwell"
date: "2024-10-24"
output: pdf_document
---

#### Flye and Medaka 

*Flye* is a de novo assembler typically used for data generated by ONT. It takes raw reads as an input and outputs polished contigs. *Medaka* is a tool used to create consensus sequences and variant calls from nanpore data. Here, I used medaka for polishing 


```{bash}

 Set variables
INPUT_FASTQ="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/all_bams/KC21Clean.q10.fastq "
GENOME_SIZE="1.3g"
THREADS=8
MEDAKA_MODEL="r1041_e82_400bps_sup_v5.0.0"

# Activate the shared environment for Flye and Medaka
echo "Activating environment..."
source activate medaka

# Run Flye
echo "Running Flye assembly..."
flye --nano-raw $INPUT_FASTQ --out-dir KC21Clean_flye_assembly --threads $THREADS --genome-size $GENOME_SIZE

echo "Assembly complete!"


#!/bin/bash

# Set up error handling
set -e

# Activate the environment 
source ~/miniforge3/etc/profile.d/conda.sh
conda activate medaka

# Set variables
INPUT_FASTQ="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/all_bams/KC21Clean.q10.fastq"
ASSEMBLY_DIR="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/all_bams/KC21Clean_flye_assembly"
ASSEMBLY_FASTA="${ASSEMBLY_DIR}/assembly.fasta"
OUTPUT_DIR="${ASSEMBLY_DIR}/medaka_output"
MEDAKA_MODEL="r1041_e82_400bps_sup_v5.0.0" 

# Create output directory if it doesn't exist
mkdir -p ${OUTPUT_DIR}

# Run Medaka polishing
echo "Running Medaka polishing..."
medaka_consensus -i ${INPUT_FASTQ} -d ${ASSEMBLY_FASTA} -o ${OUTPUT_DIR} -t 8 -m ${MEDAKA_MODEL} 2>&1 | tee ${OUTPUT_DIR}/medaka_log.txt

# Check if Medaka completed successfully
if [ $? -eq 0 ]; then
    echo "Medaka polishing completed successfully."
else
    echo "Medaka polishing failed. Check the log file for details."
    exit 1
fi

echo "Assembly and polishing complete!"

```

#### Purge Duplications 
```{bash}
#!/bin/bash

set -e  # Exit on any error

# Set variables
ASSEMBLY="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/all_bams/KC21Clean_flye_assembly/medaka_output/consensus.fasta"
READS="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/all_bams/KC21Clean.q10.fastq"
OUTPUT_DIR="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/all_bams/purge_dups_output"
PURGE_DUPS_PATH="/home/carrie/purge_dups"
THREADS=20

# Create output directory
mkdir -p $OUTPUT_DIR

# Align reads to the Medaka-polished assembly
echo "Aligning reads to assembly..."
minimap2 -x map-ont -t $THREADS $ASSEMBLY $READS | gzip -c - > $OUTPUT_DIR/aligned.paf.gz

# Check if the alignment was successful
if [ ! -s "$OUTPUT_DIR/aligned.paf.gz" ]; then
    echo "Error: Alignment file is empty or not created."
    exit 1
fi

# Produce stats and cutoffs file
echo "Generating stats and cutoffs..."
$PURGE_DUPS_PATH/bin/pbcstat $OUTPUT_DIR/aligned.paf.gz
$PURGE_DUPS_PATH/bin/calcuts PB.stat > $OUTPUT_DIR/cutoffs 2> $OUTPUT_DIR/calcults.log

# Split assembly
echo "Splitting assembly..."
$PURGE_DUPS_PATH/bin/split_fa $ASSEMBLY > $OUTPUT_DIR/assembly.split.fa
echo "Split assembly file size: $(du -h $OUTPUT_DIR/assembly.split.fa | cut -f1)"

# Check if the split was successful
if [ ! -s "$OUTPUT_DIR/assembly.split.fa" ]; then
    echo "Error: Split assembly file is empty or not created."
    exit 1
fi

# Self-align
echo "Performing self-alignment..."
minimap2 -xasm5 -DP -t $THREADS $OUTPUT_DIR/assembly.split.fa $OUTPUT_DIR/assembly.split.fa | gzip -c - > $OUTPUT_DIR/assembly.split.self.paf.gz

# Check if the self-alignment was successful
if [ ! -s "$OUTPUT_DIR/assembly.split.self.paf.gz" ]; then
    echo "Error: Self-alignment file is empty or not created."
    exit 1
fi

# Purge dups and haplotigs
echo "Purging duplicates..."
$PURGE_DUPS_PATH/bin/purge_dups -2 -T $OUTPUT_DIR/cutoffs -c PB.base.cov $OUTPUT_DIR/assembly.split.self.paf.gz > $OUTPUT_DIR/dups.bed 2> $OUTPUT_DIR/purge_dups.log

# Get purged primary and haplotigs
echo "Getting purged sequences..."
$PURGE_DUPS_PATH/bin/get_seqs -e $OUTPUT_DIR/dups.bed $ASSEMBLY

# Generate histogram
echo "Generating coverage histogram..."
$PURGE_DUPS_PATH/scripts/hist_plot.py -c $OUTPUT_DIR/cutoffs PB.stat $OUTPUT_DIR/coverage_histogram.png

echo "Purge_dups process completed. Check the output files in $OUTPUT_DIR"

```

#### Curate the Contigs 

```{bash}
#!/bin/bash

set -e  # Exit on any error

# Set variables
PURGED_ASSEMBLY="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/all_bams/purged.fa"
READS="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/all_bams/KC21Clean.q10.fastq"
OUTPUT_DIR="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/all_bams/curated_assembly"
THREADS=20

# Create output directory
mkdir -p $OUTPUT_DIR

# Step 1: Remove low fragment sizes
echo "Removing contigs smaller than 10kb..."
seqkit seq -m 10000 $PURGED_ASSEMBLY > $OUTPUT_DIR/filtered_size.fasta

# Step 2: Map reads to assembly
echo "Mapping reads to assembly..."
minimap2 -ax map-ont -t $THREADS $OUTPUT_DIR/filtered_size.fasta $READS > $OUTPUT_DIR/aligned.sam

# Step 3: Sort and index
echo "Sorting and indexing alignment..."
samtools sort -@ $THREADS $OUTPUT_DIR/aligned.sam -o $OUTPUT_DIR/aligned.bam
samtools index $OUTPUT_DIR/aligned.bam

# Step 4: Calculate coverage
echo "Calculating coverage..."
mosdepth -t $THREADS -n $OUTPUT_DIR/coverage $OUTPUT_DIR/aligned.bam

# Step 5: Filter contigs based on coverage
echo "Filtering contigs based on coverage..."
# Adjusted based on the histogram: keeping contigs between 6X and 60X
awk '$4 >= 6 && $4 <= 60 {print $1}' $OUTPUT_DIR/coverage.mosdepth.summary.txt | sort | uniq > $OUTPUT_DIR/keepers.txt

# Step 6: Extract final curated contigs
echo "Extracting final curated contigs..."
seqkit grep -f $OUTPUT_DIR/keepers.txt $OUTPUT_DIR/filtered_size.fasta -o $OUTPUT_DIR/curated.fa

echo "Contig curation complete. Final assembly: $OUTPUT_DIR/curated.fa"
```

#### Scaffolding using Rag-tag and closing the gaps
```{bash}
#!/bin/bash

# Set variables
CURATED_ASSEMBLY="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/all_bams/curated_assembly/curated.fa"
REFERENCE="/home/carrie/Desktop/Fundulus-Caren/GCF_01125445.2_MU-UCD_Fhet_4.1_genomic.fna"
READS="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/all_bams/KC21Clean.q10.fastq"
OUTPUT_DIR="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/all_bams/scaffolded_and_gapclosed"
THREADS=20

# Create output directory
mkdir -p $OUTPUT_DIR

# Activate conda environment
conda activate ragtag

# Step 1: Scaffolding with RagTag
echo "Starting scaffolding with RagTag..."
ragtag.py scaffold $REFERENCE $CURATED_ASSEMBLY \
    -o $OUTPUT_DIR/ragtag_output \
    -t $THREADS

# Step 2: Gap closing with TGS-GapCloser
echo "Starting gap closing with TGS-GapCloser..."
tgsgapcloser \
    --scaff $OUTPUT_DIR/ragtag_output/ragtag.scaffold.fasta \
    --reads $READS \
    --output $OUTPUT_DIR/gapclosed \
    --thread $THREADS \
    --ne

echo "Scaffolding and gap closing complete. Final assembly: $OUTPUT_DIR/gapclosed.scaff_seqs"

```


#### Running BUSCO

```{bash}
#!/bin/bash

# Set variables
ASSEMBLY_PRE="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/all_bams/KC21Clean_flye_assembly/medaka_output/consensus.fasta"
ASSEMBLY_POST="/path/to/your/scaffolded_assembly.fasta"  # Update this path after scaffolding
OUTPUT_DIR="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/all_bams/busco_results"
LINEAGE="actinopterygii_odb10"  # Appropriate lineage for Fundulus
THREADS=20  # Adjust based on your system's capabilities

# Create output directory
mkdir -p $OUTPUT_DIR

# Activate conda environment (if BUSCO is installed in a conda environment)
 conda activate busco_env 

# Run BUSCO on pre-scaffolding assembly
echo "Running BUSCO on pre-scaffolding assembly..."
busco -i $ASSEMBLY_PRE -o ${OUTPUT_DIR}/pre_scaffolding -m genome -l $LINEAGE -c $THREADS --force

# After scaffolding, run BUSCO again
# Uncomment the following lines after you've performed scaffolding

# echo "Running BUSCO on post-scaffolding assembly..."
# busco -i $ASSEMBLY_POST -o ${OUTPUT_DIR}/post_scaffolding -m genome -l $LINEAGE -c $THREADS --force

echo "BUSCO analysis complete. Results are in $OUTPUT_DIR"

```

#### Name the Chromosomes <3 
```{bash}
#!/bin/bash

# Set my variables 
REFERENCE_GENOME="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/all_bams/references/GCF_011125445.2_MU-UCD_Fhet_4.1_genomic.fna"
MY_GENOME="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/all_bams/scaffolded_and_gapclosed/ragtag_output/ragtag.scaffold.fasta"
OUTPUT_DIR="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/new_fundulus_genome/"


# Align genome to the reference
minimap2 -ax asm5 $REFERENCE_GENOME $MY_GENOME > alignment.sam

# Convert SAM to sorted BAM
samtools view -bS alignment.sam | samtools sort -o alignment_sorted.bam

# Extract chromosome assignments
samtools view alignment_sorted.bam | awk '{print $1, $3}' | sort -u > chr_assignments.txt

# Label chromosomes 
awk '/^>/ {
    getline seq
    name = substr($0, 2)
    cmd = "grep \"^" name " \" chr_assignments.txt"
    cmd | getline assignment
    close(cmd)
    if (assignment != "") {
        split(assignment, a)
        print ">" a[2] "_" name
    } else {
        print ">Unassigned_" name
    }
    print seq
}' $MY_GENOME > $OUTPUT_DIR/new_fundulus_genome.fasta

# Clean up temporary files
rm alignment.sam alignment_sorted.bam chr_assignments.txt

echo "Labeled genome saved as $OUTPUT_DIR/new_fundulus_genome.fasta"
echo "Nice job!!!! You're doing great! :)"
```

#### Create a chromosome mapping file and create a new GFF
```{bash}

#!/bin/bash

# Define input and output files
ALIGNMENT_BAM="alignment_sorted.bam"
OUTPUT_DIR="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/new_fundulus_genome/"

# Extract unique scaffold-to-chromosome mappings
samtools view $ALIGNMENT_BAM | awk '{print $1, $3}' | sort -u > $OUTPUT_DIR/chromosome_map.txt

echo "Chromosome map created: $MAPPING_FILE"

# Set variables
EXISTING_GFF="/home/carrie/Fundulus-Caren/Ready_to_process/all_bams/reference/GCF_011125445.2_MU-UCD_Fhet_4.1_genomic.gff"
NEW_GENOME="/home/carrie/Fundulus-Caren/Ready_to_process/new_fudnulus_genome/new_fundulus_genome.fasta"
CHROMOSOME_MAP="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/new_fundulus_genome/chromosome_map.txt"
OUTPUT_GFF="/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/new_fundulus_genome/updated_fundulus.gff"

# Update chromosome names
gffread $EXISTING_GFF -m $CHROMOSOME_MAP -o temp_updated.gff

# Ensure features align with the new genome
gffread temp_updated.gff -g $NEW_GENOME -O -o $OUTPUT_GFF

# Clean up
rm temp_updated.gff

echo "Updated GFF file created, go get 'em: $OUTPUT_GFF"

```

#### GeMoMa to annotate the Genome! 
The output is the final_annotation.gff. This can be used 
```{bash}
# Install GeMoMa and its dependencies
#conda create -n gemoma_env
conda init
conda activate gemoma_env
#conda install -c bioconda -c conda-forge gemoma mmseqs2 busco

# Prepare to run GeMoMa
#echo "Fundulus heteroclitus" > fundulus-species.txt
#mkdir -p references

# Copy the directories <3 
#cp /home/carrie/Desktop/Fundulus-Caren/GCF_011125445.2_MU-UCD_Fhet_4.1_genomic.fna references/
#cp /home/carrie/Downloads/GCF_011125445.2_MU-UCD_Fhet_4.1_genomic.gff.gz references/

# Run GeMoMa
GeMoMa GeMoMaPipeline \
  threads=20 \
  outdir=/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/ \
  GeMoMa.Score=ReAlign \
  AnnotationFinalizer.r=NO \
  o=true \
  t=/home/carrie/Desktop/Fundulus-Caren/Ready_to_process/all_bams/scaffolded_and_gapclosed/ragtag_output/ragtag.scaffold.fasta \
  a=references/GCF_011125445.2_MU-UCD_Fhet_4.1_genomic.gff.gz \
  g=references/GCF_011125445.2_MU-UCD_Fhet_4.1_genomic.fna \
  -Xms5G -Xmx64G
```

#### Liftover coordinates from the NCBI reference to the new reference genome

# Create a whole genome alignmentusing lastz
# currently checking to see if KegAlign will work better than Lastz. Lastz was updated in 2022. This appears to be a "precise genome aligner efficiently leveraging GPUs".
```{bash}
lastz GCF_011125445.2_MU-UCD_Fhet_4.1_genomic.fna new_fundulus_genome.fa --chain --format=axt >alignment.axt
```

# Convert the alignment to chain format

```{bash}
axtChain -linearGap=medium alignment.axt NCBI_reference.fa your_new_genome.fa alignment.chain
chainNet alignment.chain NCBI_reference.sizes your_new_genome.sizes alignment.net
netChainSubset alignment.net alignment.chain alignment.liftover.chain
```

### Identifying new ROI coordinates in my new_funduls_genome based on NCBI fundulus genome

#Pre-processing step: Grabbing the ROIs_genes and processigng hte GFF file to extract what I want
```{bash}
#!/bin/bash

GFF_FILE="path/to/your/fundulus_genome.gff"
GENE_LIST="roi_genes.txt"
OUTPUT_BED="roi_coordinates.bed"
NOT_FOUND_FILE="genes_not_found.txt"

# Create associative arrays
declare -A genes
declare -A found_genes

# Read gene list into array
while IFS= read -r gene; do
    genes["$gene"]=1
done < "$GENE_LIST"

# Extract gene coordinates from GFF file and mark found genes
awk -F'\t' 'BEGIN{OFS="\t"}
    $3=="gene" {
        split($9,a,";");
        for(i in a) {
            if(a[i] ~ /^Name=/) {
                gsub("Name=", "", a[i]);
                if(a[i] in ENVIRON) {
                    print $1, $4-1, $5, a[i];
                    print a[i] > "/dev/stderr";
                }
            }
        }
    }
' "${!genes[@]}" "$GFF_FILE" > "$OUTPUT_BED" 2> >(sort -u > found_genes_temp.txt)

# Read found genes into array
while IFS= read -r gene; do
    found_genes["$gene"]=1
done < found_genes_temp.txt

# Find and print genes not detected
not_found_count=0
> "$NOT_FOUND_FILE"  # Clear the file if it exists
for gene in "${!genes[@]}"; do
    if [[ ! ${found_genes["$gene"]} ]]; then
        echo "$gene" >> "$NOT_FOUND_FILE"
        ((not_found_count++))
    fi
done

# Add the count to the not found file
echo -e "\nTotal genes not found: $not_found_count" >> "$NOT_FOUND_FILE"

# Clean up temporary file
rm found_genes_temp.txt

echo "Coordinates for found genes are in $OUTPUT_BED"
echo "Genes not found are listed in $NOT_FOUND_FILE"
echo "Total genes not found: $not_found_count"

```


#Step 1: Exract the ROI sequences from the NCBI genome
```{bash}
bedtools getfasta -fi ncbi_fundulus_genome.ga -bed roi.bed -name -fo roi_sequences.fa
```

#Step 2: Blast the ROI against my genome
```{bash}
# Create BLAST database for your custom genome
makeblastdb -in your_custom_fundulus_genome.fa -dbtype nucl -out custom_fundulus_db

# Check to make sure the blast database looks great (it does- this will print all the infromation about the new database including the total bases)
blastdbcmd -db /home/tia/Desktop/new_fundulus_genome/tias_fundulus_genome_db - info

# Perform BLAST search
blastn -query roi_sequences.fa -db custom_fundulus_db -outfmt "6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore" -out blast_results.txt
```

#Step 3: Parse BLAST results and extract new coordinates
```{bash}
#!/bin/bash

input_file="AHR_blast_results.txt"
output_file="new_roi_coordinates.tsv"

# Write header
echo -e "Gene_Name\tChromosome\tStart\tEnd" > "$output_file"

# Process the input file
while IFS=$'\t' read -r col1 col2 col3 col4 col5 col6 col7 col8 col9 col10 col11 col12; do
    gene_name=$(echo "$col1" | cut -d':' -f1)
    chromosome="$col2"
    start=$(( col9 < col10 ? col9 : col10 ))
    end=$(( col9 > col10 ? col9 : col10 ))
    
    echo -e "$gene_name\t$chromosome\t$start\t$end" >> "$output_file"
done < "$input_file"

```

# Step 4: Filter out top hits only
```{bash}
#!/bin/bash

# Input and output file names
BLAST_RESULTS="AHR_blast_results.txt"  # Input BLAST results file
FILTERED_RESULTS="AHR_filtered_results_top_hits.txt"  # Output file for top hits

# Check if the input file exists
if [[ ! -f "$BLAST_RESULTS" ]]; then
    echo "Error: BLAST results file '$BLAST_RESULTS' not found!"
    exit 1
fi

# Sort by query ID (column 1) and bit score (column 12, descending), then filter top hits
sort -k1,1 -k12,12nr "$BLAST_RESULTS" | awk '!seen[$1]++' > "$FILTERED_RESULTS"

# Check if the filtering was successful
if [[ $? -eq 0 ]]; then
    echo "Filtered results saved to '$FILTERED_RESULTS'."
else
    echo "Error: Something went wrong during filtering."
fi

```

### Create a GFF file for the fundulus genome using gmap_build

```{bash}
# Build a GMAP database for your new genome
gmap_build -d new_genome_db /path/to/new_genome.fasta

# Extract gene/transcript sequences from old genome
gffread -w old_genes.fa -g /path/to/old_genome.fasta /path/to/old_annotations.gff

# Map to new genome and generate GFF3
gmap -d new_genome_db -f gff3_gene old_genes.fa > new_annotations.gff3

```